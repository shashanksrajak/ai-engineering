{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f397df69",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "\n",
    "Backward propagration is main algo for neural net training and pytorch does it with its autograd engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5919db9",
   "metadata": {},
   "source": [
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b7e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293e9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc87922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x123f073d0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x123f05600>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c81c99",
   "metadata": {},
   "source": [
    "### Compute Gradients\n",
    "\n",
    "In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the requires_grad property of those tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d23f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of w now :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"grad of w now : \", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce0339d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of w now :  tensor([[0.0329, 0.2865, 0.0714],\n",
      "        [0.0329, 0.2865, 0.0714],\n",
      "        [0.0329, 0.2865, 0.0714],\n",
      "        [0.0329, 0.2865, 0.0714],\n",
      "        [0.0329, 0.2865, 0.0714]])\n",
      "grad of b now :  tensor([0.0329, 0.2865, 0.0714])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()  # Computes the gradient of current tensor wrt graph leaves.\n",
    "print(\"grad of w now : \", w.grad)\n",
    "print(\"grad of b now : \", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd8945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad # this will be done as require_grad was not set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
