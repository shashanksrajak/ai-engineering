{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d59902",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "Handling gradients in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "347441a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e527bd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = x^2\n",
    "# dy_dx = 2*x\n",
    "\n",
    "def dy_dx(x):\n",
    "    return 2*x\n",
    "\n",
    "dy_dx(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b137fb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.466781571308061"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z = sin(y) \n",
    "# chain rule -> dz_dx = dz_dy * dy_dx\n",
    "# dz_dy = cos(y) = cos(x^2)\n",
    "\n",
    "def dz_dx(x):\n",
    "    return 2*x*math.cos(x**2)\n",
    "\n",
    "dz_dx(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0891dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -[ylog(y_hat)+(1-y)log(1-y_hat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf4ac6",
   "metadata": {},
   "source": [
    "### How to use Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc0528",
   "metadata": {},
   "source": [
    "We create a `tensor` with `requires_grad` as `True`. \n",
    "This signals pytorch that whenever we perform any operation on this tensor, it will keep its record and calculate its gradient and store it.\n",
    "And when we need this gradient, it simply returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2185418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True) #requires_grad is False by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d4a4b",
   "metadata": {},
   "source": [
    "When we do this next ops, torch will create a computation graph, where x is multiplied with x and generates y. Hence,`dy_dx` will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c9d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e79b3ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd0f96ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ae93406",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c0317c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4121, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.sin(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dc9673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bf91418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "420bdd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d2/38ffj459225g1hwkswmksdxm0000gp/T/ipykernel_60266/486760323.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e016a",
   "metadata": {},
   "source": [
    "### Training a simple binary classifier manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76459950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs \n",
    "x = torch.tensor(6.7) # cgpa\n",
    "y = torch.tensor(0.0) # not placed\n",
    "\n",
    "# parameters\n",
    "w = torch.tensor(1.0) #weight\n",
    "b = torch.tensor(0.0) #bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d006d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy for loss function\n",
    "def binary_cross_entropy_loss(y_predicted, y_target):\n",
    "    epsilon = 1e-8 # to prevent log(0)\n",
    "\n",
    "    y_predicted = torch.clamp(y_predicted, epsilon, 1-epsilon)\n",
    "\n",
    "    return -(y_target*torch.log(y_predicted) + (1-y_target)*torch.log(1-y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e2ab079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "# 1. linear transform\n",
    "z = w * x + b\n",
    "\n",
    "# 2. sigmoid\n",
    "y_predicted = torch.sigmoid(z)\n",
    "\n",
    "# 3. compute loss\n",
    "loss = binary_cross_entropy_loss(y_predicted, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cea936a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e5f4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "\n",
    "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
    "dloss_dy_pred = (y_predicted - y)/(y_predicted* (1-y_predicted) )\n",
    "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
    "dy_pred_dz = y_predicted * (1 - y_predicted)\n",
    "# 3. dz/dw and dz/db: z with respect to w and b\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "# dz/dw = x|\n",
    "# dz/db = 1 (bias contributes directly to z)\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "908c4a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient of loss wrt weight dw: 6.691762447357178\n",
      "Manual gradient of loss wrt bias db: 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual gradient of loss wrt weight dw: {dL_dw}\")\n",
    "print(f\"Manual gradient of loss wrt bias db: {dL_db}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616ceb0",
   "metadata": {},
   "source": [
    "### Doing the same work but with coolness of PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b450289",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3e065",
   "metadata": {},
   "source": [
    "Note, that here we will keep `requires_grad` as `True` for w and b as we need their gradients actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba9cdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edc0d7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w*x + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "043c1998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = torch.sigmoid(z)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36ae0231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = binary_cross_entropy_loss(y_predicted, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30cce2",
   "metadata": {},
   "source": [
    "Now we will simply use `backward()` and the computation graph will do its magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e351b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbb8383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17581eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5738db",
   "metadata": {},
   "source": [
    "### Be aware of Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d127de05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "572eb1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "696408a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36401c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07e2f9",
   "metadata": {},
   "source": [
    "after calc grad once, if you do forward pass again and then again backward then the grad of x will increment (accumulate) so we need to clear the gradients before \n",
    "running the next forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fee88be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99cb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.requires_grad_(False) \n",
    "# use this when grad needs to be turned off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-codelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
